<!doctype html><html lang=en dir=ltr><head><meta charset=utf-8><meta name=viewport content="height=device-height,width=device-width,initial-scale=1,minimum-scale=1"><meta name=generator content="Hugo 0.121.1"><meta name=generator content="Relearn 5.23.2+tip"><meta name=robots content="noindex, nofollow, noarchive, noimageindex"><meta name=description content="Kubernetes security pocket gude and more"><meta name=author content="Ansil H"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://cks.ansilh.com/images/hero.png"><meta name=twitter:title content><meta name=twitter:description content="Kubernetes security pocket gude and more"><meta property="og:title" content><meta property="og:description" content="Kubernetes security pocket gude and more"><meta property="og:type" content="website"><meta property="og:url" content="https://cks.ansilh.com/index.html"><meta property="og:image" content="https://cks.ansilh.com/images/hero.png"><meta property="og:site_name" content="Hugo Relearn Theme"><title></title>
<link href=./images/favicon.png?1745965898 rel=icon type=image/png><link href=./css/fontawesome-all.min.css?1745965898 rel=stylesheet media=print onload='this.media="all",this.onload=null'><noscript><link href=./css/fontawesome-all.min.css?1745965898 rel=stylesheet></noscript><link href=./css/nucleus.css?1745965898 rel=stylesheet><link href=./css/auto-complete.css?1745965898 rel=stylesheet media=print onload='this.media="all",this.onload=null'><noscript><link href=./css/auto-complete.css?1745965898 rel=stylesheet></noscript><link href=./css/perfect-scrollbar.min.css?1745965898 rel=stylesheet><link href=./css/fonts.css?1745965898 rel=stylesheet media=print onload='this.media="all",this.onload=null'><noscript><link href=./css/fonts.css?1745965898 rel=stylesheet></noscript><link href=./css/theme.css?1745965898 rel=stylesheet><link href=./css/theme-zen-dark.css?1745965898 rel=stylesheet id=R-variant-style><link href=./css/chroma-relearn-dark.css?1745965898 rel=stylesheet id=R-variant-chroma-style><link href=./css/variant.css?1745965898 rel=stylesheet><link href=./css/print.css?1745965898 rel=stylesheet media=print><link href=./css/format-print.css?1745965898 rel=stylesheet><link href=./css/ie.css?1745965898 rel=stylesheet><script src=./js/url.js?1745965898></script><script src=./js/variant.js?1745965898></script><script>window.index_js_url="./index.search.js";var root_url="./",baseUri=root_url.replace(/\/$/,"");window.relearn=window.relearn||{},window.relearn.baseUriFull="https://cks.ansilh.com/",window.relearn.themeVariantModifier="",window.variants&&variants.init(["zen-dark"]),window.T_Copy_to_clipboard=`Copy to clipboard`,window.T_Copied_to_clipboard=`Copied to clipboard!`,window.T_Copy_link_to_clipboard=`Copy link to clipboard`,window.T_Link_copied_to_clipboard=`Copied link to clipboard!`,window.T_Reset_view=`Reset view`,window.T_View_reset=`View reset!`,window.T_No_results_found=`No results found for "{0}"`,window.T_N_results_found=`{1} results found for "{0}"`</script><style>#R-body img.bg-white{background-color:#fff}</style></head><body class="mobile-support print disableInlineCopyToClipboard" data-url=./index.html><div id=R-body class=default-animation><div id=R-body-overlay></div><nav id=R-topbar><div class=topbar-wrapper><div class=topbar-sidebar-divider></div><div class="topbar-area topbar-area-start" data-area=start><div class="topbar-button topbar-button-sidebar" data-content-empty=disable data-width-s=show data-width-m=hide data-width-l=hide><button class=topbar-control onclick=toggleNav() type=button title="Menu (CTRL+ALT+n)"><i class="fa-fw fas fa-bars"></i></button></div></div><ol class="topbar-breadcrumbs breadcrumbs highlightable" itemscope itemtype=http://schema.org/BreadcrumbList></ol><div class="topbar-area topbar-area-end" data-area=end></div></div></nav><div id=R-main-overlay></div><main id=R-body-inner class="highlightable home" tabindex=-1><div class=flex-block-wrapper><section><h1 class=a11y-only>Subsections of</h1><article class=chapter><header class=headline></header><div class=article-subheading>Chapter 1</div><h1 id=introduction>Introduction</h1><h4 id=certified-kubernetes-security-specialist-cks-exam-curriculum---v131>Certified Kubernetes Security Specialist (CKS) Exam Curriculum - v1.31</h4><div style=float:left><h3>15% - Cluster Setup</h3><ul><li>Use Network security policies to restrict cluster level access<li>Use CIS benchmark to review the security configuration of Kubernetes components (etcd,
kubelet, kubedns, kubeapi)<li>Properly set up Ingress objects with TLS<li>Protect node metadata and endpoints<li>Verify platform binaries before deploying</ul><h3>15% - Cluster Hardening</h3><ul><li>Use Role Based Access Controls to minimize exposure<li>Exercise caution in using service accounts e.g. disable defaults, minimize permissions on
newly created ones<li>Restrict access to Kubernetes API<li>Upgrade Kubernetes to avoid vulnerabilities</ul><h3>10% - System Hardening</h3><ul><li>Minimize host OS footprint (reduce attack surface)<li>Using least-privilege identity and access management<li>Minimize external access to the network<li>Appropriately use kernel hardening tools such as AppArmor, seccomp</ul><h3>20% - Minimize Microservice Vulnerabilities</h3><ul><li>Use appropriate pod security standards<li>Manage kubernetes secrets<li>Understand and implement isolation techniques (multi-tenancy, sandboxed containers,
etc.)<li>Implement Pod-to-Pod encryption using Cilium</ul><h3>20% - Supply Chain Security</h3><ul><li>Minimize base image footprint<li>Understand your supply chain (e.g. SBOM, CI/CD, artifact repositories)<li>Secure your supply chain (permitted registries, sign and validate artifacts, etc.)<li>Perform static analysis of user workloads and container images (e.g. Kubesec, KubeLinter)</ul><h3>20% - Monitoring, Logging and Runtime Security</h3><ul><li>Perform behavioral analytics to detect malicious activities<li>Detect threats within physical infrastructure, apps, networks, data, users and workloads<li>Investigate and identify phases of attack and bad actors within the environment<li>Ensure immutability of containers at runtime<li>Use Kubernetes audit logs to monitor access</ul></div><footer class=footline></footer></article><article class=chapter><header class=headline></header><div class=article-subheading>Chapter 2</div><h1 id=lab-setup>Lab Setup</h1><h4 id=cluster-setup>Cluster Setup</h4><p>In this session, we will explore how to setup a Kubernetes cluster with one master and one worker using VirtualBox</p><h3 id=pre-requisites>Pre-requisites</h3><ul><li>Linux Desktop with Internet</li><li>VirtualBox</li></ul><footer class=footline></footer></article><section><h1 class=a11y-only>Subsections of Lab Setup</h1><article class=default><header class=headline></header><h1 id=setting-up-vms>Setting up VMs</h1><p>In this chapter, we will import Ubuntu 24.04 OVA appliance to VirtualBox and will set it up.</p><ul><li>Download Ubuntu OVA file</li></ul><div class="wrap-code highlight"><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=nb>cd</span> ~/Downloads/
</span></span><span class=line><span class=cl>curl -LO https://cloud-images.ubuntu.com/noble/current/noble-server-cloudimg-amd64.ova</span></span></code></pre></div><ul><li><p>Create a seed iso which is needed for setting initial password and login</p><ul><li>Generate new SSH keys of not already done<div class="wrap-code highlight"><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>ssh-keygen -t ed25519 -C <span class=s2>&#34;</span><span class=si>${</span><span class=nv>USER</span><span class=si>}</span><span class=s2>@{HOSTNAME}&#34;</span></span></span></code></pre></div></li><li>Install cloud-utils<div class="wrap-code highlight"><pre tabindex=0><code>sudo apt install cloud-utils</code></pre></div></li><li>Create config to set at intial-boot<div class="wrap-code highlight"><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>ls -l <span class=si>${</span><span class=nv>HOME</span><span class=si>}</span>/.ssh/*.pub
</span></span><span class=line><span class=cl>cat <span class=s>&lt;&lt;EOF &gt; my-cloud-config.yaml
</span></span></span><span class=line><span class=cl><span class=s>#cloud-config
</span></span></span><span class=line><span class=cl><span class=s>chpasswd:
</span></span></span><span class=line><span class=cl><span class=s>list: |
</span></span></span><span class=line><span class=cl><span class=s>  ubuntu:ubuntu
</span></span></span><span class=line><span class=cl><span class=s>  expire: False
</span></span></span><span class=line><span class=cl><span class=s>ssh_pwauth: True
</span></span></span><span class=line><span class=cl><span class=s>ssh_authorized_keys: 
</span></span></span><span class=line><span class=cl><span class=s>- $(cat ${HOME}/.ssh/id_ed25519.pub)
</span></span></span><span class=line><span class=cl><span class=s>EOF</span></span></span></code></pre></div></li><li>Create seed ISO<div class="wrap-code highlight"><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>cloud-localds my-seed.iso my-cloud-config.yaml</span></span></code></pre></div></li></ul></li><li><p>Import OVA</p><ul><li><img src=./02-lab_setup/01-os-installation/vb-1.png alt class="figure-image nobg-white noborder nolightbox shadow" style=height:auto;width:auto loading=lazy>
<img src=./02-lab_setup/01-os-installation/vb-2.png alt class="figure-image nobg-white noborder nolightbox shadow" style=height:auto;width:auto loading=lazy>
<img src=./02-lab_setup/01-os-installation/vb-3.png alt class="figure-image nobg-white noborder nolightbox shadow" style=height:auto;width:auto loading=lazy>
<img src=./02-lab_setup/01-os-installation/vb-4.png alt class="figure-image nobg-white noborder nolightbox shadow" style=height:auto;width:auto loading=lazy>
<img src=./02-lab_setup/01-os-installation/vb-5.png alt class="figure-image nobg-white noborder nolightbox shadow" style=height:auto;width:auto loading=lazy>
<img src=./02-lab_setup/01-os-installation/vb-6.png alt class="figure-image nobg-white noborder nolightbox shadow" style=height:auto;width:auto loading=lazy>
<img src=./02-lab_setup/01-os-installation/vb-7.png alt class="figure-image nobg-white noborder nolightbox shadow" style=height:auto;width:auto loading=lazy>
<img src=./02-lab_setup/01-os-installation/vb-8.png alt class="figure-image nobg-white noborder nolightbox shadow" style=height:auto;width:auto loading=lazy>
<img src=./02-lab_setup/01-os-installation/vb-9.png alt class="figure-image nobg-white noborder nolightbox shadow" style=height:auto;width:auto loading=lazy></li></ul></li><li><p>Repeat the same steps for a worker node</p></li><li><p>Power-on the VMs and wait for the prompt to set new password</p><ul><li><p>Note:- Password is &ldquo;ubuntu&rdquo;</p><p><img src=./02-lab_setup/01-os-installation/vb-10.png alt class="figure-image nobg-white noborder nolightbox shadow" style=height:auto;width:auto loading=lazy></p></li></ul></li><li><p>Since you are connected to the same network where wifi is available, IP address will be set on both VMs.</p><ul><li><p>Execute <code>ip a</code> on each VMs to see the IP</p></li><li><p><img src=./02-lab_setup/01-os-installation/vb-11.png alt class="figure-image nobg-white noborder nolightbox shadow" style=height:auto;width:auto loading=lazy></p></li></ul></li><li><p>Now you can SSH into the nodes from you local system using the IP and the credentials.</p></li><li><p>Better setup password-less auth</p><div class="wrap-code highlight"><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>ssh-copy-id ubuntu@192.168.0.175
</span></span><span class=line><span class=cl>ssh-copy-id ubuntu@192.168.0.149</span></span></code></pre></div></li><li><p>Set hostnames</p><div class="wrap-code highlight"><pre tabindex=0><code>sudo hostnamectl set-hostname cks-master</code></pre></div><div class="wrap-code highlight"><pre tabindex=0><code>sudo hostnamectl set-hostname cks-worker</code></pre></div></li></ul><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=install-containerd>Install containerd</h1><p>In this chapter, we will install the container runtime</p><ul><li><p>Install containerd</p><div class="wrap-code highlight"><pre tabindex=0><code># Add Docker&#39;s official GPG key:
sudo apt-get update
sudo apt-get install ca-certificates curl
sudo install -m 0755 -d /etc/apt/keyrings
sudo curl -fsSL https://download.docker.com/linux/ubuntu/gpg -o /etc/apt/keyrings/docker.asc
sudo chmod a+r /etc/apt/keyrings/docker.asc

# Add the repository to Apt sources:
echo \
&#34;deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.asc] https://download.docker.com/linux/ubuntu \
$(. /etc/os-release &amp;&amp; echo &#34;${UBUNTU_CODENAME:-$VERSION_CODENAME}&#34;) stable&#34; | \
sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/null
sudo apt-get update</code></pre></div><div class="wrap-code highlight"><pre tabindex=0><code>sudo apt-get install containerd.io</code></pre></div></li><li><p>Create default containerd config file</p><div class="wrap-code highlight"><pre tabindex=0><code>containerd config default | sudo tee /etc/containerd/config.toml</code></pre></div></li><li><p>Enable Cgroup to get support for Cgroup v2</p><div class="wrap-code highlight"><pre tabindex=0><code>sudo sed -i &#39;s/SystemdCgroup = false/SystemdCgroup = true/&#39; /etc/containerd/config.toml</code></pre></div></li><li><p>Restart containerd</p><div class="wrap-code highlight"><pre tabindex=0><code>sudo systemctl restart containerd
sudo systemctl status containerd</code></pre></div><p>Output:-</p><div class="wrap-code highlight"><pre tabindex=0><code>● containerd.service - containerd container runtime
    Loaded: loaded (/usr/lib/systemd/system/containerd.service; enabled; preset: enabled)
    Active: active (running) since Wed 2025-02-05 21:47:38 UTC; 32s ago
    Docs: https://containerd.io
    Process: 2555 ExecStartPre=/sbin/modprobe overlay (code=exited, status=0/SUCCESS)
Main PID: 2558 (containerd)
    Tasks: 9
    Memory: 13.6M (peak: 14.1M)
        CPU: 162ms
    CGroup: /system.slice/containerd.service
            └─2558 /usr/bin/containerd

Feb 05 21:47:38 cks-master containerd[2558]: time=&#34;2025-02-05T21:47:38.695180848Z&#34; level=info msg=&#34;Start subscribing containerd event&#34;
Feb 05 21:47:38 cks-master containerd[2558]: time=&#34;2025-02-05T21:47:38.695912510Z&#34; level=info msg=&#34;Start recovering state&#34;
Feb 05 21:47:38 cks-master containerd[2558]: time=&#34;2025-02-05T21:47:38.696006958Z&#34; level=info msg=&#34;Start event monitor&#34;
Feb 05 21:47:38 cks-master containerd[2558]: time=&#34;2025-02-05T21:47:38.696017639Z&#34; level=info msg=&#34;Start snapshots syncer&#34;
Feb 05 21:47:38 cks-master containerd[2558]: time=&#34;2025-02-05T21:47:38.696025262Z&#34; level=info msg=&#34;Start cni network conf syncer for default&#34;
Feb 05 21:47:38 cks-master containerd[2558]: time=&#34;2025-02-05T21:47:38.696035741Z&#34; level=info msg=&#34;Start streaming server&#34;
Feb 05 21:47:38 cks-master containerd[2558]: time=&#34;2025-02-05T21:47:38.696123579Z&#34; level=info msg=serving... address=/run/containerd/containerd.sock.ttrpc
Feb 05 21:47:38 cks-master containerd[2558]: time=&#34;2025-02-05T21:47:38.696159432Z&#34; level=info msg=serving... address=/run/containerd/containerd.sock
Feb 05 21:47:38 cks-master containerd[2558]: time=&#34;2025-02-05T21:47:38.696201277Z&#34; level=info msg=&#34;containerd successfully booted in 0.021733s&#34;
Feb 05 21:47:38 cks-master systemd[1]: Started containerd.service - containerd container runtime.</code></pre></div></li><li><p>Enable packet forwarding</p><div class="wrap-code highlight"><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=c1># sysctl params required by setup, params persist across reboots</span>
</span></span><span class=line><span class=cl>cat <span class=s>&lt;&lt;EOF | sudo tee /etc/sysctl.d/k8s.conf
</span></span></span><span class=line><span class=cl><span class=s>net.ipv4.ip_forward = 1
</span></span></span><span class=line><span class=cl><span class=s>EOF</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Apply sysctl params without reboot</span>
</span></span><span class=line><span class=cl>sudo sysctl --system </span></span></code></pre></div></li></ul><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=install-and-configure-kubeadm-and-kubelet>Install and configure kubeadm and kubelet</h1><p>In this chapter, we will install and configure kubeadm, kubelet and kubectl</p><p>These instructions are for Kubernetes v1.31.</p><ul><li><p>Install curl</p><div class="wrap-code highlight"><pre tabindex=0><code>sudo apt-get update
# apt-transport-https may be a dummy package; if so, you can skip that package
sudo apt-get install -y apt-transport-https ca-certificates curl gpg</code></pre></div></li><li><p>Install keys needed for k8s packages</p><div class="wrap-code highlight"><pre tabindex=0><code>curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.31/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg</code></pre></div></li><li><p>Add the appropriate Kubernetes apt repository</p><div class="wrap-code highlight"><pre tabindex=0><code># This overwrites any existing configuration in /etc/apt/sources.list.d/kubernetes.list
echo &#39;deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.31/deb/ /&#39; | sudo tee /etc/apt/sources.list.d/kubernetes.list</code></pre></div></li><li><p>Update the apt package index, install kubelet, kubeadm and kubectl, and pin their version:</p><div class="wrap-code highlight"><pre tabindex=0><code>sudo apt-get update
sudo apt-get install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl</code></pre></div></li><li><p>Enable kubelet</p><div class="wrap-code highlight"><pre tabindex=0><code>sudo systemctl enable --now kubelet</code></pre></div></li></ul><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=initialize-cluster>Initialize Cluster</h1><p>In this chapter, we will initialize the master node and then will add the worker to the cluster</p><p>Logon to the master node and follow along</p><ul><li>Initialize kubeadm<div class="wrap-code highlight"><pre tabindex=0><code>sudo kubeadm init --ignore-preflight-errors=NumCPU --pod-network-cidr 10.0.0.0/16</code></pre></div>Output:-</li></ul><div class="wrap-code highlight"><pre tabindex=0><code>I0205 22:31:24.915138    4255 version.go:261] remote version is much newer: v1.32.1; falling back to: stable-1.31
[init] Using Kubernetes version: v1.31.5
[preflight] Running pre-flight checks
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action beforehand using &#39;kubeadm config images pull&#39;
W0205 22:31:25.854623    4255 checks.go:846] detected that the sandbox image &#34;registry.k8s.io/pause:3.8&#34; of the container runtime is inconsistent with that used by kubeadm.It is recommended to use &#34;registry.k8s.io/pause:3.10&#34; as the CRI sandbox image.
[certs] Using certificateDir folder &#34;/etc/kubernetes/pki&#34;
[certs] Generating &#34;ca&#34; certificate and key
[certs] Generating &#34;apiserver&#34; certificate and key
[certs] apiserver serving cert is signed for DNS names [cks-master kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 192.168.0.175]
[certs] Generating &#34;apiserver-kubelet-client&#34; certificate and key
[certs] Generating &#34;front-proxy-ca&#34; certificate and key
[certs] Generating &#34;front-proxy-client&#34; certificate and key
[certs] Generating &#34;etcd/ca&#34; certificate and key
[certs] Generating &#34;etcd/server&#34; certificate and key
[certs] etcd/server serving cert is signed for DNS names [cks-master localhost] and IPs [192.168.0.175 127.0.0.1 ::1]
[certs] Generating &#34;etcd/peer&#34; certificate and key
[certs] etcd/peer serving cert is signed for DNS names [cks-master localhost] and IPs [192.168.0.175 127.0.0.1 ::1]
[certs] Generating &#34;etcd/healthcheck-client&#34; certificate and key
[certs] Generating &#34;apiserver-etcd-client&#34; certificate and key
[certs] Generating &#34;sa&#34; key and public key
[kubeconfig] Using kubeconfig folder &#34;/etc/kubernetes&#34;
[kubeconfig] Writing &#34;admin.conf&#34; kubeconfig file
[kubeconfig] Writing &#34;super-admin.conf&#34; kubeconfig file
[kubeconfig] Writing &#34;kubelet.conf&#34; kubeconfig file
[kubeconfig] Writing &#34;controller-manager.conf&#34; kubeconfig file
[kubeconfig] Writing &#34;scheduler.conf&#34; kubeconfig file
[etcd] Creating static Pod manifest for local etcd in &#34;/etc/kubernetes/manifests&#34;
[control-plane] Using manifest folder &#34;/etc/kubernetes/manifests&#34;
[control-plane] Creating static Pod manifest for &#34;kube-apiserver&#34;
[control-plane] Creating static Pod manifest for &#34;kube-controller-manager&#34;
[control-plane] Creating static Pod manifest for &#34;kube-scheduler&#34;
[kubelet-start] Writing kubelet environment file with flags to file &#34;/var/lib/kubelet/kubeadm-flags.env&#34;
[kubelet-start] Writing kubelet configuration to file &#34;/var/lib/kubelet/config.yaml&#34;
[kubelet-start] Starting the kubelet
[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory &#34;/etc/kubernetes/manifests&#34;
[kubelet-check] Waiting for a healthy kubelet at http://127.0.0.1:10248/healthz. This can take up to 4m0s
[kubelet-check] The kubelet is healthy after 1.004888923s
[api-check] Waiting for a healthy API server. This can take up to 4m0s
[api-check] The API server is healthy after 8.502467445s
[upload-config] Storing the configuration used in ConfigMap &#34;kubeadm-config&#34; in the &#34;kube-system&#34; Namespace
[kubelet] Creating a ConfigMap &#34;kubelet-config&#34; in namespace kube-system with the configuration for the kubelets in the cluster
[upload-certs] Skipping phase. Please see --upload-certs
[mark-control-plane] Marking the node cks-master as control-plane by adding the labels: [node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]
[mark-control-plane] Marking the node cks-master as control-plane by adding the taints [node-role.kubernetes.io/control-plane:NoSchedule]
[bootstrap-token] Using token: zcn9eo.k3bc7xk2hoz2k37h
[bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
[bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to get nodes
[bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
[bootstrap-token] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
[bootstrap-token] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
[bootstrap-token] Creating the &#34;cluster-info&#34; ConfigMap in the &#34;kube-public&#34; namespace
[kubelet-finalize] Updating &#34;/etc/kubernetes/kubelet.conf&#34; to point to a rotatable kubelet client certificate and key
[addons] Applied essential addon: CoreDNS
[addons] Applied essential addon: kube-proxy

Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

Alternatively, if you are the root user, you can run:

  export KUBECONFIG=/etc/kubernetes/admin.conf

You should now deploy a pod network to the cluster.
Run &#34;kubectl apply -f [podnetwork].yaml&#34; with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join 192.168.0.175:6443 --token zcn9eo.k3bc7xk2hoz2k37h \
	--discovery-token-ca-cert-hash sha256:9a5c622b76b969b6ebe1f80ac903f738237625c7a0375a707466dbe6e724d261 </code></pre></div><ul><li><p>Now logon to the worker node and execute the join command from the output</p><div class="wrap-code highlight"><pre tabindex=0><code>sudo kubeadm join 192.168.0.175:6443 --token zcn9eo.k3bc7xk2hoz2k37h \
    --discovery-token-ca-cert-hash sha256:9a5c622b76b969b6ebe1f80ac903f738237625c7a0375a707466dbe6e724d261 </code></pre></div></li><li><p>Verify nodes</p><div class="wrap-code highlight"><pre tabindex=0><code>mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config
kubectl get nodes</code></pre></div><p>Output: -</p><div class="wrap-code highlight"><pre tabindex=0><code>ubuntu@cks-master:~$ kubectl get nodes
NAME         STATUS     ROLES           AGE     VERSION
cks-master   NotReady   control-plane   2m23s   v1.31.5
cks-worker   NotReady   &lt;none&gt;          90s     v1.31.5</code></pre></div><p>The nodes are in <code>NotReady</code> state because we didn&rsquo;t install CNI.</p></li></ul><p>In the next chapter, we will install Cilium CNI. Stay tuned!</p><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=install-cni---cilium>Install CNI - Cilium</h1><p>In this chapter, we will deploy Cilium CNI</p><p>You need to execute these steps only from master node</p><ul><li><p>Download the cilium CLI</p><div class="wrap-code highlight"><pre tabindex=0><code>CILIUM_CLI_VERSION=$(curl -s https://raw.githubusercontent.com/cilium/cilium-cli/main/stable.txt)
CLI_ARCH=amd64
if [ &#34;$(uname -m)&#34; = &#34;aarch64&#34; ]; then CLI_ARCH=arm64; fi
curl -L --fail --remote-name-all https://github.com/cilium/cilium-cli/releases/download/${CILIUM_CLI_VERSION}/cilium-linux-${CLI_ARCH}.tar.gz{,.sha256sum}
sha256sum --check cilium-linux-${CLI_ARCH}.tar.gz.sha256sum
sudo tar xzvfC cilium-linux-${CLI_ARCH}.tar.gz /usr/local/bin
rm cilium-linux-${CLI_ARCH}.tar.gz{,.sha256sum}</code></pre></div></li><li><p>Deploy CNI</p><div class="wrap-code highlight"><pre tabindex=0><code>cilium install --version 1.17.0</code></pre></div></li><li><p>Check the deployment status (It may take 10-15mins depends on your internet speed)</p><div class="wrap-code highlight"><pre tabindex=0><code>cilium status --wait</code></pre></div></li><li><p>Once deployment completes the output will show all in good shape like below.</p><div class="wrap-code highlight"><pre tabindex=0><code>    /¯¯\
 /¯¯\__/¯¯\    Cilium:             OK
 \__/¯¯\__/    Operator:           OK
 /¯¯\__/¯¯\    Envoy DaemonSet:    OK
 \__/¯¯\__/    Hubble Relay:       disabled
    \__/       ClusterMesh:        disabled

DaemonSet              cilium             Desired: 2, Ready: 2/2, Available: 2/2
DaemonSet              cilium-envoy       Desired: 2, Ready: 2/2, Available: 2/2
Deployment             cilium-operator    Desired: 1, Ready: 1/1, Available: 1/1
Containers:            cilium             Running: 2
                    cilium-envoy       Running: 2
                    cilium-operator    Running: 1
Cluster Pods:          2/2 managed by Cilium
Helm chart version:    1.17.0
Image versions         cilium             quay.io/cilium/cilium:v1.17.0@sha256:51f21bdd003c3975b5aaaf41bd21aee23cc08f44efaa27effc91c621bc9d8b1d: 2
                    cilium-envoy       quay.io/cilium/cilium-envoy:v1.31.5-1737535524-fe8efeb16a7d233bffd05af9ea53599340d3f18e@sha256:57a3aa6355a3223da360395e3a109802867ff635cb852aa0afe03ec7bf04e545: 2
                    cilium-operator    quay.io/cilium/operator-generic:v1.17.0@sha256:1ce5a5a287166fc70b6a5ced3990aaa442496242d1d4930b5a3125e44cccdca8: 1</code></pre></div></li><li><p>Delete the cilium operator to cleanup transient errors that may get flagged during test</p><div class="wrap-code highlight"><pre tabindex=0><code>kubectl delete pods -n kube-system -l name=cilium-operator</code></pre></div></li><li><p>You can run a connectivity test to verify the kubernetes network health</p><div class="wrap-code highlight"><pre tabindex=0><code>cilium connectivity test</code></pre></div><p>After a while, you should see below output</p><div class="wrap-code highlight"><pre tabindex=0><code>...
ℹ️  Single-node environment detected, enabling single-node connectivity test
ℹ️  Monitor aggregation detected, will skip some flow validation steps
⌛ [kubernetes] Waiting for deployment cilium-test-1/client to become ready...
⌛ [kubernetes] Waiting for deployment cilium-test-1/client2 to become ready...
⌛ [kubernetes] Waiting for deployment cilium-test-1/echo-same-node to become ready...
⌛ [kubernetes] Waiting for pod cilium-test-1/client-b65598b6f-n99h7 to reach DNS server on cilium-test-1/echo-same-node-5c4dc4674d-7vtmj pod...
⌛ [kubernetes] Waiting for pod cilium-test-1/client2-84576868b4-6xjp5 to reach DNS server on cilium-test-1/echo-same-node-5c4dc4674d-7vtmj pod...
⌛ [kubernetes] Waiting for pod cilium-test-1/client-b65598b6f-n99h7 to reach default/kubernetes service...
⌛ [kubernetes] Waiting for pod cilium-test-1/client2-84576868b4-6xjp5 to reach default/kubernetes service...
⌛ [kubernetes] Waiting for Service cilium-test-1/echo-same-node to become ready...
⌛ [kubernetes] Waiting for Service cilium-test-1/echo-same-node to be synchronized by Cilium pod kube-system/cilium-vlnmr
⌛ [kubernetes] Waiting for NodePort 192.168.0.175:32072 (cilium-test-1/echo-same-node) to become ready...
⌛ [kubernetes] Waiting for NodePort 192.168.0.149:32072 (cilium-test-1/echo-same-node) to become ready...
..............
.. redacted..
..............
✅ [cilium-test-1] All 66 tests (275 actions) successful, 44 tests skipped, 1 scenarios skipped.</code></pre></div></li><li><p>Now cluster is ready!</p><div class="wrap-code highlight"><pre tabindex=0><code>ubuntu@cks-master:~$ kubectl get nodes
NAME         STATUS   ROLES           AGE   VERSION
cks-master   Ready    control-plane   32m   v1.31.5
cks-worker   Ready    &lt;none&gt;          31m   v1.31.5
ubuntu@cks-master:~$ </code></pre></div></li></ul><p>Congratulations on setting up your cluster!
Now we can start practicing our topics. See you there.</p><footer class=footline></footer></article></section><article class=default><header class=headline></header><h1 id=rbac>RBAC</h1><footer class=footline></footer></article><section><h1 class=a11y-only>Subsections of RBAC</h1><article class=default><header class=headline></header><h1 id=service-accounts>Service Accounts</h1><p>The basic unit in Kubernetes is a Pod.
Pods runs inside a namespace.
Every namespace have a default service account.</p><p>Why we need service account?</p><p>If your workload needs to access the Kubernetes API, then we need a mechanism to authenticate and authorize the request.
Here we can use the service account and assign permissions to it.</p><p>This service account can then be used inside a Pod.</p><p>Every namespace have a <code>default</code> service account.</p><div class="wrap-code highlight"><pre tabindex=0><code>ubuntu@cks-master:~$ kubectl get sa
NAME      SECRETS   AGE
default   0         4d19h
ubuntu@cks-master:~$ </code></pre></div><h2 id=service-account>Service account</h2><div class="wrap-code highlight"><pre tabindex=0><code>kubectl create -f - &lt;&lt;EOF
apiVersion: v1
kind: ServiceAccount
metadata:
  name: mysvcacc
  namespace: default
EOF</code></pre></div><p>Create a Pod and use this service account to contact the API</p><div class="wrap-code highlight"><pre tabindex=0><code>kubectl create -f - &lt;&lt;EOF
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: api-test
  name: api-test
spec:
  serviceAccountName: mysvcacc
  containers:
  - image: nginx
    name: api-test
EOF</code></pre></div><p>The authentication using service account happens via a signed JWT token which gets mounted inside the Pod.</p><p>View token</p><div class="wrap-code highlight"><pre tabindex=0><code>kubectl exec -it api-test -- cat /run/secrets/kubernetes.io/serviceaccount/token &amp;&amp; echo</code></pre></div><p>You can visit the <a href=https://jwt.io/ target=_blank>https://jwt.io/</a> to and paste the token to see the details.</p><p><img src=./03-rbac/01-sevice-accounts/token.png alt class="figure-image nobg-white noborder nolightbox shadow" style=height:auto;width:auto loading=lazy></p><p>As the screenshot shows, the API service can indentify the token and handle authorization using the &ldquo;sub&rdquo; field.</p><p>You can control the token mount behavior using below flag.</p><div class="wrap-code highlight"><pre tabindex=0><code>FIELD: automountServiceAccountToken &lt;boolean&gt;


DESCRIPTION:
    AutomountServiceAccountToken indicates whether pods running as this service
    account should have an API token automatically mounted. Can be overridden at
    the pod level.</code></pre></div><p>In which scenario you need to set <code>automountServiceAccountToken</code> to false.</p><p>The scenario which your workload don&rsquo;t have to contact API, but need a service account.</p><p>Yes, you can use service account for another scenario where you need an imagePullSecret to pull container images.</p><h2 id=using-imagepullsecter-in-serviceaccount>Using imagePullSecter in ServiceAccount</h2><div class="wrap-code highlight"><pre tabindex=0><code>kubectl create secret docker-registry myregistrykey --docker-server=&lt;registry name&gt; \
        --docker-username=DUMMY_USERNAME --docker-password=DUMMY_DOCKER_PASSWORD \
        --docker-email=DUMMY_DOCKER_EMAIL</code></pre></div><div class="wrap-code highlight"><pre tabindex=0><code>kubectl patch serviceaccount mysvcacc -p &#39;{&#34;imagePullSecrets&#34;: [{&#34;name&#34;: &#34;myregistrykey&#34;}]}&#39;</code></pre></div><div class="wrap-code highlight"><pre tabindex=0><code>apiVersion: v1
kind: ServiceAccount
metadata:
  creationTimestamp: 2021-07-07T22:02:39Z
  name: mysvcacc
  namespace: default
  uid: 052fb0f4-3d50-11e5-b066-42010af0d7b6
imagePullSecrets:
  - name: myregistrykey</code></pre></div><h2 id=accessing-api-from-pod>Accessing API from Pod</h2><div class="wrap-code highlight"><pre tabindex=0><code>curl -k  https://${KUBERNETES_SERVICE_HOST}/api -H &#34;Authorization: Bearer $(cat /run/secrets/kubernetes.io/serviceaccount/token)&#34;</code></pre></div><div class="wrap-code highlight"><pre tabindex=0><code>{
  &#34;kind&#34;: &#34;APIVersions&#34;,
  &#34;versions&#34;: [
    &#34;v1&#34;
  ],
  &#34;serverAddressByClientCIDRs&#34;: [
    {
      &#34;clientCIDR&#34;: &#34;0.0.0.0/0&#34;,
      &#34;serverAddress&#34;: &#34;192.168.0.175:6443&#34;
    }
  ]
}</code></pre></div><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=role-and-clusterrole>Role and ClusterRole</h1><p>The <code>Role</code> is for a namespace and <code>ClusterRole</code> is for entire cluster.</p><p>A role maps the permissions to an object.</p><p>Below role indicates get and list permissions to object Pod inside the namespace default.</p><div class="wrap-code highlight"><pre tabindex=0><code>kubectl create role myaccrole --verb=get --verb=list --resource=pod --dry-run=client -o yaml</code></pre></div><div class="wrap-code highlight"><pre tabindex=0><code>apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  creationTimestamp: null
  name: myaccrole
  namespace: default
rules:
- apiGroups:
  - &#34;&#34;
  resources:
  - pods
  verbs:
  - get
  - list</code></pre></div><p>Below ClusterRole is same as Role, but the difference here is that the ClusterRole can be assigned to many namespaces.</p><div class="wrap-code highlight"><pre tabindex=0><code>kubectl create clusterrole myaccrole --verb=get --verb=list --resource=pod --dry-run=client -o yaml</code></pre></div><div class="wrap-code highlight"><pre tabindex=0><code>apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  creationTimestamp: null
  name: myaccrole
rules:
- apiGroups:
  - &#34;&#34;
  resources:
  - pods
  verbs:
  - get
  - list</code></pre></div><footer class=footline></footer></article><article class=default><header class=headline></header><h1 id=rolebinding-and-clusterrolebinding>RoleBinding and ClusterRoleBinding</h1><p>To bind a role to &ldquo;User&rdquo;, &ldquo;Group&rdquo;, or &ldquo;ServiceAccount&rdquo;</p><p>I below example, we will give pod read/list access to service account mysvcacc</p><div class="wrap-code highlight"><pre tabindex=0><code>kubectl create rolebinding  mysvcacc --role=myaccrole --serviceaccount=default:mysvcacc --dry-run=client -o yaml</code></pre></div><div class="wrap-code highlight"><pre tabindex=0><code>apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  creationTimestamp: null
  name: mysvcacc
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: myaccrole
subjects:
- kind: ServiceAccount
  name: mysvcacc
  namespace: default</code></pre></div><footer class=footline></footer></article></section><article class=default><header class=headline></header><h1 id=network-policy>Network Policy</h1><p><a href=https://github.com/ahmetb/kubernetes-network-policy-recipes target=_blank>https://github.com/ahmetb/kubernetes-network-policy-recipes</a></p><footer class=footline></footer></article></section></div></main></div><script src=./js/clipboard.min.js?1745965898 defer></script><script src=./js/perfect-scrollbar.min.js?1745965898 defer></script><script src=./js/theme.js?1745965898 defer></script></body></html>